<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />

    

    
    <title>python小说爬虫 | Hexo</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="keywords" content="python,爬虫" />
    
    <meta name="description" content="前言很久没有写爬虫了，最近接到一个抓取小说的项目顺便做此纪录练练手，之后工作中可能也会有部分场景要用到爬虫，爬取竞争对手进行数据分析什么的。目标网站：潇湘书院环境准备：  python3 requests库 BeautifulSoup库  整体思路抓取这个小说网站免费板块的所有内容，查看页面发现这个板块一共有6697页，每页有20本小说，那整体思路就是先抓取每页的20个小说名称和url，然后进入每">
<meta property="og:type" content="article">
<meta property="og:title" content="python小说爬虫">
<meta property="og:url" content="http://yoursite.com/2018/04/25/python%E5%B0%8F%E8%AF%B4%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="前言很久没有写爬虫了，最近接到一个抓取小说的项目顺便做此纪录练练手，之后工作中可能也会有部分场景要用到爬虫，爬取竞争对手进行数据分析什么的。目标网站：潇湘书院环境准备：  python3 requests库 BeautifulSoup库  整体思路抓取这个小说网站免费板块的所有内容，查看页面发现这个板块一共有6697页，每页有20本小说，那整体思路就是先抓取每页的20个小说名称和url，然后进入每">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://my.lipeilipei.top/18-4-26/72083805.jpg">
<meta property="article:published_time" content="2018-04-25T12:30:00.000Z">
<meta property="article:modified_time" content="2019-11-05T14:07:34.701Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="python">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://my.lipeilipei.top/18-4-26/72083805.jpg">
    

    

    
        <link rel="icon" href="/css/images/bitbug_favicon.ico" />
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/titillium-web/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">


    
<script src="/libs/jquery/3.5.0/jquery.min.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
    
        <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?8229091b2a281af4b398716fce50a2cb";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    
    


<meta name="generator" content="Hexo 4.2.1"></head>

<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                    <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Web%E5%BC%80%E5%8F%91/">Web开发</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E5%AE%89%E5%85%A8/">安全</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%B5%8B%E8%AF%95/">测试</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E8%87%AA%E5%8A%A8%E5%8C%96/">自动化</a></li></ul>
                                
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <article id="post-python小说爬虫" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        python小说爬虫
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
  <div class="article-date">
    <i class="fa fa-calendar"></i>
    <a href="/2018/04/25/python%E5%B0%8F%E8%AF%B4%E7%88%AC%E8%99%AB/" class="article-date">
       <time datetime="2018-04-25T12:30:00.000Z" itemprop="datePublished">2018-04-25</time>
    </a>
  </div>


<div class="article-date">
  <i class="fa fa-calendar-plus-o"></i>
  <a href="/2018/04/25/python%E5%B0%8F%E8%AF%B4%E7%88%AC%E8%99%AB/" class="article-date">
     <time datetime="2019-11-05T14:07:34.701Z" itemprop="dateModified">2019-11-05</time>
  </a>
</div>


                

                
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/python/" rel="tag">python</a>, <a class="tag-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a>
    </div>

                
   <span id="/2018/04/25/python%E5%B0%8F%E8%AF%B4%E7%88%AC%E8%99%AB/" class="leancloud-visitors" data-flag-title="python小说爬虫">
   <i class="fa fa-eye"></i>
   <span class="post-meta-item-text">Views: </span>
   <i class="leancloud-visitors-count">1000000</i>
   </span>


                

            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            

            

            

            <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>很久没有写爬虫了，最近接到一个抓取小说的项目顺便做此纪录练练手，之后工作中可能也会有部分场景要用到爬虫，爬取竞争对手进行数据分析什么的。<br>目标网站：<a href="http://www.xxsy.net/" target="_blank" rel="noopener"><strong>潇湘书院</strong></a><br>环境准备：</p>
<ul>
<li>python3</li>
<li>requests库</li>
<li>BeautifulSoup库</li>
</ul>
<h2 id="整体思路"><a href="#整体思路" class="headerlink" title="整体思路"></a>整体思路</h2><p>抓取这个小说网站免费板块的所有内容，查看页面发现这个板块一共有6697页，每页有20本小说，那整体思路就是先抓取每页的20个小说名称和url，然后进入每本小说的阅读地址，拿到每一个章节的标题和url，抓取每章节的正文内容并写到本地txt文本中。<br><img src="123" alt=""></p>
<h2 id="单页分析"><a href="#单页分析" class="headerlink" title="单页分析"></a>单页分析</h2><p>这里请求使用requests，解析页面用非常方便的BeautifulSoup，在一个文章标题上右键检查，在高亮的这条a标签右键，copy selector，通过这条selector来定位BeautifulSoup解析后小说所在的位置。<br><img src="http://my.lipeilipei.top/18-4-26/95059112.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">body &gt; div.content &gt; div &gt; div &gt; div.inner-mainbar &gt; div.search-result &gt; div.result-list &gt; ul &gt; li:nth-child(1) &gt; div &gt; h4 &gt; a</span><br></pre></td></tr></table></figure>
<p>其中<code>li:nth-child(1)</code>很明显是指这个小说在当前页面小说列表中排列第一个，我们想要本页面所有的20本小说，所以就删掉这个<code>:nth-child(1)</code>，再将selector语句精简一下只要能定位到即可，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">div.result-list &gt; ul &gt; li &gt; div &gt; h4 &gt; a</span><br></pre></td></tr></table></figure>
<p>然后将它写入代码，print查看一下结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests,os</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">webdata = requests.get(<span class="string">'http://www.xxsy.net/search?vip=0&amp;sort=2'</span>)</span><br><span class="line">soup = BeautifulSoup(webdata.text,<span class="string">'lxml'</span>)</span><br><span class="line">books = soup.select(<span class="string">'div.result-list &gt; ul &gt; li &gt; div &gt; h4 &gt; a'</span>)</span><br><span class="line">print(books)</span><br></pre></td></tr></table></figure>
<p><img src="http://my.lipeilipei.top/18-4-26/26352205.jpg" alt=""><br>结果是一个列表，列表中有20个元素，分别对应的20个小说，我们想要的是每个元素中href后面的链接和小说的名字，用循环提取出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">for</span> book <span class="keyword">in</span> books:</span><br><span class="line">    bookname = book.text</span><br><span class="line">    bookurl = <span class="string">'http://www.xxsy.net'</span> + book.get(<span class="string">'href'</span>)</span><br><span class="line">    bookid = book.get(<span class="string">'href'</span>).split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">    print(bookname,bookurl,bookid)</span><br></pre></td></tr></table></figure>
<p><img src="http://my.lipeilipei.top/18-4-26/86095250.jpg" alt=""><br>这样就很简单的拿到了单个页面20本小说的标题和url地址，可以看到上面我把地址中的一串数字用单独取出来了，这个数字其实就是小说对应的唯一ID，后面要用到。</p>
<h2 id="章节分析"><a href="#章节分析" class="headerlink" title="章节分析"></a>章节分析</h2><p>点击一个小说进入详情页面，切换至”作品目录”下可以看到所有的章节，这时候在一个章节上右键检查，跟上面方法一样，同样可以很简单的获取到该小说所有章节的名称和url地址以及章节ID。<br><img src="http://my.lipeilipei.top/18-4-26/51873134.jpg" alt=""></p>
<p>为了在一篇文章介绍尽可能多的方法，这里我不用上面这个地方来获取章节信息，从另一个地方进入。点击“开始阅读”进入第一章节的正文，在页面的左边可以看到一个“目录”按钮，点看以后就能看到所有的章节名称了。然而在这些章节名称上右键发现当前页面禁用了鼠标右键功能，无法检查元素当然也就不能copy它的selector。<br><img src="http://my.lipeilipei.top/18-4-26/63797573.jpg" alt=""></p>
<p>这种按钮的点击肯定是向服务器发送了请求的，打开fiddler进行抓包，再次点击目录按钮，此时可以看到这个请求已经被成功捕获，点击这条请求查看详细信息，这个接口的功能就是查询所有的章节信息。<br><img src="http://my.lipeilipei.top/18-4-26/17982944.jpg" alt=""></p>
<p>真实的请求地址是<code>http://www.xxsy.net/partview/GetChapterListNoSub?bookid=945608&amp;isvip=0</code>，其中bookid就是我们前面找到的bookid，这里可以做成参数化依次传入其他小说的ID。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">url = <span class="string">'http://www.xxsy.net/partview/GetChapterListNoSub?bookid=945608&amp;isvip=0'</span></span><br><span class="line">titles = BeautifulSoup(requests.get(url).text,<span class="string">'lxml'</span>).select(<span class="string">'ul &gt; li &gt; a'</span>)</span><br><span class="line"><span class="keyword">for</span> title <span class="keyword">in</span> titles:</span><br><span class="line">    titlename = title.text  <span class="comment">#章节名称</span></span><br><span class="line">    titleurl = <span class="string">'http://www.xxsy.net'</span> + title.get(<span class="string">'href'</span>)   <span class="comment">#章节地址</span></span><br><span class="line">    print(titlename,titleurl)</span><br></pre></td></tr></table></figure>
<p><img src="http://my.lipeilipei.top/18-4-26/53687066.jpg" alt=""></p>
<h2 id="正文下载"><a href="#正文下载" class="headerlink" title="正文下载"></a>正文下载</h2><p>上一步取到的titleurl是每一章节的阅读地址，直接requests请求并解析拿到正文内容</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">contents = BeautifulSoup(requests.get(titleurl).text, <span class="string">'lxml'</span>).select(<span class="string">'div#auto-chapter &gt; p'</span>)   <span class="comment">#正文内容</span></span><br></pre></td></tr></table></figure>
<p><img src="http://my.lipeilipei.top/18-4-26/83734461.jpg" alt=""><br>返回结果是一个列表，每一个元素是一个段落，将每一段内容前后无用的标签剔除并写到本地的txt文本，写入方式为<code>a+</code>,每次写入时在后面追加，不会覆盖之前的内容，文本自动按照前面获取到的小说名来命名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">    content = str(content).replace(<span class="string">'&lt;p&gt;'</span>, <span class="string">'\n'</span>).replace(<span class="string">'&lt;/p&gt;'</span>, <span class="string">''</span>)</span><br><span class="line">    f = open(path + <span class="string">'%s.txt'</span> % bookname, <span class="string">'a+'</span>)</span><br><span class="line">    f.write(content)</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure>
<p><img src="http://my.lipeilipei.top/18-4-26/31951196.jpg" alt=""></p>
<p>到此为止三个步骤已经完成，可以顺利的爬下一个章节的内容了，现在通过几个for循环将这几个步骤整合，就可以源源不断的开始下载小说了。<br><img src="http://my.lipeilipei.top/18-4-26/272032.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> requests,time,os</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">path = <span class="string">r'C:\Users\lipei\Desktop\潇湘书院爬虫\小说\\'</span>               <span class="comment">#本地存放小说的路径</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_books</span><span class="params">(url)</span>:</span></span><br><span class="line">    webdata = requests.get(url,timeout=<span class="number">60</span>)   </span><br><span class="line">    soup = BeautifulSoup(webdata.text,<span class="string">'lxml'</span>)</span><br><span class="line">    books = soup.select(<span class="string">'div.result-list &gt; ul &gt; li &gt; div &gt; h4 &gt; a'</span>)</span><br><span class="line">    <span class="keyword">for</span> book <span class="keyword">in</span> books:</span><br><span class="line">        bookid = book.get(<span class="string">'href'</span>).split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'.'</span>)[<span class="number">0</span>]   <span class="comment">#小说ID作为下一个请求ur中的参数</span></span><br><span class="line">        bookname = book.text                                     <span class="comment">#小说名称</span></span><br><span class="line">        bookurl = <span class="string">'http://www.xxsy.net'</span> + book.get(<span class="string">'href'</span>)       <span class="comment">#小说url地址</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> bookname+<span class="string">'.txt'</span> <span class="keyword">in</span> oldlists:                          <span class="comment">#判断是否已经下载过，若存在则跳过</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">'=====================正在下载【'</span> + bookname + <span class="string">'】====================='</span>)</span><br><span class="line">        url = <span class="string">'http://www.xxsy.net/partview/GetChapterListNoSub?bookid=%s&amp;isvip=0'</span> % bookid</span><br><span class="line">        titles = BeautifulSoup(requests.get(url,timeout=<span class="number">60</span>).text,<span class="string">'lxml'</span>).select(<span class="string">'ul &gt; li &gt; a'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> title <span class="keyword">in</span> titles:</span><br><span class="line">            titleurl = <span class="string">'http://www.xxsy.net'</span> + title.get(<span class="string">'href'</span>)  <span class="comment">#章节url地址</span></span><br><span class="line">            titlename = title.text                                <span class="comment">#章节名称</span></span><br><span class="line">            <span class="comment"># print(titlename,titleurl)</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                f = open(path + <span class="string">'%s.txt'</span> % bookname, <span class="string">'a+'</span>)        <span class="comment">#章节名称写到txt文本</span></span><br><span class="line">                f.write(<span class="string">'\n'</span>*<span class="number">2</span> + titlename + <span class="string">'\n'</span>)</span><br><span class="line">                f.close()</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">            contents = BeautifulSoup(requests.get(titleurl,timeout=<span class="number">60</span>).text, <span class="string">'lxml'</span>).select(<span class="string">'div#auto-chapter &gt; p'</span>)</span><br><span class="line">            <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">                content = str(content).replace(<span class="string">'&lt;p&gt;'</span>, <span class="string">'\n'</span>).replace(<span class="string">'&lt;/p&gt;'</span>, <span class="string">''</span>)</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    f = open(path + <span class="string">'%s.txt'</span> % bookname, <span class="string">'a+'</span>)   <span class="comment">#正文内容卸载txt文本，紧接在章节名称的下面</span></span><br><span class="line">                    f.write(content)</span><br><span class="line">                    f.close()</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    <span class="keyword">pass</span></span><br><span class="line">            print(titlename + <span class="string">'[已下载]'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    urls = [<span class="string">'http://www.xxsy.net/search?vip=0&amp;sort=2&amp;pn=&#123;&#125;'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6697</span>)]   <span class="comment">#免费板块每一页的url通过最后的pn参数控制</span></span><br><span class="line">    <span class="keyword">global</span> oldlists</span><br><span class="line">    oldlists = os.listdir(path)        <span class="comment">#爬虫开始之前检查当前目录已有的文件</span></span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">        get_books(url)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#控制每天爬取的数量，达到要求后停止任务，通过任务开始前后目录中的文件数量相减来判断</span></span><br><span class="line">        newlists = os.listdir(path)</span><br><span class="line">        num = len(newlists) - len(oldlists)</span><br><span class="line">        <span class="keyword">if</span> num &gt;= <span class="number">20</span>:</span><br><span class="line">            print(<span class="string">'今日任务下载完毕，今日下载小说%d本'</span> % num)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>在测试过程中发现短时间持续请求该网站的话会可能被服务器拒绝，但是并没有封禁IP，这可能是这个网站唯一的反爬措施了，然而并没有卵用，将请求放在一个无限循环里面，若被拒绝就自动重连，连上以后跳出循环。<br>后面如果遇到封禁IP的网站再讲如何通过更换IP来规避。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        webdata = requests.get(url,timeout=<span class="number">60</span>)</span><br><span class="line">        <span class="keyword">break</span>      <span class="comment">#连接成功就跳出循环</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        time.sleep(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><font color="white">peili</font></p>

        </div>
        <footer class="article-footer">
            

    <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more">分享到：</a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间">QQ空间</a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博">新浪微博</a>
    <a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博">腾讯微博</a>
    <a href="#" class="bds_renren" data-cmd="renren" title="分享到人人网">人人网</a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信">微信</a>
</div>
<script>
window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"share":{"bdSize":16}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>
<style>
    .bdshare_popup_box {
        border-radius: 4px;
        border: #e1e1e1 solid 1px;
    }
    .bdshare-button-style0-16 a,
    .bdshare-button-style0-16 .bds_more {
        padding-left: 20px;
        margin: 6px 10px 6px 0;
    }
    .bdshare_dialog_list a,
    .bdshare_popup_list a,
    .bdshare_popup_bottom a {
        font-family: 'Microsoft Yahei';
    }
    .bdshare_popup_top {
        display: none;
    }
    .bdshare_popup_bottom {
        height: auto;
        padding: 5px;
    }
</style>



        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "John Doe"
        },
        "headline": "python小说爬虫",
        "image": "http://yoursite.comhttp://my.lipeilipei.top/18-4-26/72083805.jpg",
        "keywords": "python 爬虫",
        "genre": "爬虫",
        "datePublished": "2018-04-25",
        "dateCreated": "2018-04-25",
        "dateModified": "2019-11-05",
        "url": "http://yoursite.com/2018/04/25/python小说爬虫/",
        "description": "前言很久没有写爬虫了，最近接到一个抓取小说的项目顺便做此纪录练练手，之后工作中可能也会有部分场景要用到爬虫，爬取竞争对手进行数据分析什么的。目标网站：潇湘书院环境准备：

python3
requests库
BeautifulSoup库

整体思路抓取这个小说网站免费板块的所有内容，查看页面发现这个板块一共有6697页，每页有20本小说，那整体思路就是先抓取每页的20个小说名称和url，然后进入每",
        "wordCount": 1371
    }
</script>

</article>

    <section id="comments">
    
        
    <!-- Valine -->
    <div class="vcomments"></div>


    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/peilipeili" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2018/04/27/requests%E4%B8%ADHTTPS%E8%AF%B7%E6%B1%82%E7%9A%84%E5%A4%84%E7%90%86/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            requests中HTTPS请求的处理
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2018/03/28/python%E6%93%8D%E4%BD%9Ctxt%E6%96%87%E6%9C%AC%E7%9A%84%E5%87%A0%E7%A7%8D%E6%A8%A1%E5%BC%8F/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">python操作txt文本的几种模式</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="">
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/09/24/%E6%9F%90%E5%8D%9A%E5%BD%A9%E7%BD%91%E7%AB%99%E6%B8%97%E9%80%8F%E5%AE%9E%E6%88%98/" class="thumbnail">
    
    
        <span style="background-image:url(http://my.lipeilipei.top/images/2020/20200922131655.png)" alt="某博彩网站渗透实战" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E5%AE%89%E5%85%A8/">安全</a></p>
                            <p class="item-title"><a href="/2020/09/24/%E6%9F%90%E5%8D%9A%E5%BD%A9%E7%BD%91%E7%AB%99%E6%B8%97%E9%80%8F%E5%AE%9E%E6%88%98/" class="title">某博彩网站渗透实战</a></p>
                            <p class="item-date"><time datetime="2020-09-24T04:00:00.000Z" itemprop="datePublished">2020-09-24</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/09/13/net%20user%20%E8%A2%AB%E6%9D%80%E8%BD%AF%E6%8B%A6%E6%88%AA%E7%BB%95%E8%BF%87%E6%96%B9%E5%BC%8F/" class="thumbnail">
    
    
        <span style="background-image:url(http://my.lipeilipei.top/images/2020/net_%E6%8B%A6%E6%88%AA.gif)" alt="net user 被杀软拦截绕过方式" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E5%AE%89%E5%85%A8/">安全</a></p>
                            <p class="item-title"><a href="/2020/09/13/net%20user%20%E8%A2%AB%E6%9D%80%E8%BD%AF%E6%8B%A6%E6%88%AA%E7%BB%95%E8%BF%87%E6%96%B9%E5%BC%8F/" class="title">net user 被杀软拦截绕过方式</a></p>
                            <p class="item-date"><time datetime="2020-09-13T04:00:00.000Z" itemprop="datePublished">2020-09-13</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/08/20/%E9%80%9A%E8%BE%BEOA%20v11.6%20%E4%BB%BB%E6%84%8F%E6%96%87%E4%BB%B6%E5%88%A0%E9%99%A4+%E4%BB%BB%E6%84%8F%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E6%BC%8F%E6%B4%9E/" class="thumbnail">
    
    
        <span style="background-image:url(http://my.lipeilipei.top/images/2020/20200820163910.png)" alt="通达OA v11.6 任意文件删除+任意文件上传漏洞" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E5%AE%89%E5%85%A8/">安全</a></p>
                            <p class="item-title"><a href="/2020/08/20/%E9%80%9A%E8%BE%BEOA%20v11.6%20%E4%BB%BB%E6%84%8F%E6%96%87%E4%BB%B6%E5%88%A0%E9%99%A4+%E4%BB%BB%E6%84%8F%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E6%BC%8F%E6%B4%9E/" class="title">通达OA v11.6 任意文件删除+任意文件上传漏洞</a></p>
                            <p class="item-date"><time datetime="2020-08-20T04:00:00.000Z" itemprop="datePublished">2020-08-20</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/07/03/Windows%E4%B8%BB%E6%9C%BA%E6%B8%97%E9%80%8F-MS08-067/" class="thumbnail">
    
    
        <span style="background-image:url(http://my.lipeilipei.top//2020img/TIM图片20200705104958.png)" alt="windows主机渗透 - MS08-067" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E5%AE%89%E5%85%A8/">安全</a></p>
                            <p class="item-title"><a href="/2020/07/03/Windows%E4%B8%BB%E6%9C%BA%E6%B8%97%E9%80%8F-MS08-067/" class="title">windows主机渗透 - MS08-067</a></p>
                            <p class="item-date"><time datetime="2020-07-03T04:00:00.000Z" itemprop="datePublished">2020-07-03</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/05/29/Python%E6%89%93%E9%80%A0%E7%AB%AF%E5%8F%A3%E6%89%AB%E6%8F%8F%E5%99%A8(2)%20-%20TCP%20SYN%E6%89%AB%E6%8F%8F/" class="thumbnail">
    
    
        <span style="background-image:url(http://my.lipeilipei.top/20200526145544.png)" alt="Python打造端口扫描器(2) - TCP SYN扫描" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E5%AE%89%E5%85%A8/">安全</a></p>
                            <p class="item-title"><a href="/2020/05/29/Python%E6%89%93%E9%80%A0%E7%AB%AF%E5%8F%A3%E6%89%AB%E6%8F%8F%E5%99%A8(2)%20-%20TCP%20SYN%E6%89%AB%E6%8F%8F/" class="title">Python打造端口扫描器(2) - TCP SYN扫描</a></p>
                            <p class="item-date"><time datetime="2020-05-29T04:00:00.000Z" itemprop="datePublished">2020-05-29</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
        
    </div>
</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 John Doe</p>
                
                <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="https://github.com/ppoffice" target="_blank">PPOffice</a></p>
                
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

    </div>
    
    
    <script src="//unpkg.com/valine"></script>
    <script>
        var GUEST = ['nick','mail','link'];
        var meta = '';
        meta = meta.split(',').filter(function (item) {
            return GUEST.indexOf(item)>-1;
        });
        var avatarcdn = 'https://gravatar.loli.net/avatar/' == true;
        new Valine({
            el: '.vcomments',
            notify: "true",
            verify: "false",
            appId: "xte3AlbJVwsDXOptfeAOGivl-MdYXbMMI",
            appKey: "yPJe4NIicPjj3EmTGBE98PAu",
            placeholder: "建议在上方留下昵称和邮箱，以便收到回复通知",
            avatar:"monsterid",
            recordIP:"",
            visitor: "true"
        });
    </script>





    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    

    
    
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


</body>
</html>
